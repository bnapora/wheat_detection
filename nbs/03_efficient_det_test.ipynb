{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.vision.all import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\n",
    "from effdet.efficientdet import HeadNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = get_efficientdet_config('tf_efficientdet_d0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientDet(model_config,pretrained_backbone=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config.num_classes = 1\n",
    "model_config.image_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.class_net = HeadNet(model_config,num_outputs=model_config.num_classes,norm_kwargs=dict(eps=.001,momentum=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/home/heye0507/wheat_detection/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fastai bbox style is [x1,y1,x2,y2], given coco-style is [x1,y1,w,h]\n",
    "def convert_fastai_bbox(box):\n",
    "    x1,y1,w,h = box\n",
    "    x2 = x1 + w\n",
    "    y2 = y1 + h\n",
    "    return [x1,y1,x2,y2]\n",
    "    \n",
    "def prep_bbox(df):\n",
    "    images, bbox = df['image_id'],df['bbox']\n",
    "    d = collections.defaultdict(list)\n",
    "    for i,b in zip(images,bbox):\n",
    "        d[i].append(convert_fastai_bbox([float(i) for i in b[1:-1].split(',')]))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path,sz=224,bs=64):\n",
    "    base = Path(path).parent\n",
    "    df = pd.read_csv(path)\n",
    "    img2bbox = prep_bbox(df)\n",
    "    dblocks = DataBlock(\n",
    "        blocks = (ImageBlock,BBoxBlock,BBoxLblBlock),\n",
    "        splitter = RandomSplitter(valid_pct=0.),\n",
    "        get_x = lambda o: str(base/'train')+'/'+o+'.jpg',\n",
    "        get_y = [lambda o: img2bbox[o], lambda o: ['wheat' for i in range(len(img2bbox[o]))]],\n",
    "        item_tfms = Resize(sz),\n",
    "        n_inp=1\n",
    "    )\n",
    "    return dblocks.dataloaders(img2bbox.keys(),bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = get_data(path/'arvalis_1.csv',sz=256,bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "3"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(torch.Size([4, 3, 256, 256]), torch.Size([4, 61, 4]), torch.Size([4, 61]))"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "b[0].shape,b[1].shape,b[2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unscale\n",
    "1. fastai bbox is (-1,1)\n",
    "2. one batch is a tuple with (image, bbox, bboxLabel)\n",
    "3. unscale on bbox should call on b[1][0:bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def un_pad(boxes,labels):\n",
    "    bb_keep = ((boxes[:,2] - boxes[:,0])>0).nonzero()[:,0]\n",
    "    return boxes[bb_keep],labels[bb_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.vision.core import _unscale_pnts\n",
    "from effdet.anchors import Anchors,AnchorLabeler,generate_detections,MAX_DETECTION_POINTS\n",
    "from effdet.loss import DetectionLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = Anchors(\n",
    "    model_config.min_level,model_config.max_level,\n",
    "    model_config.num_scales, model_config.aspect_ratios,\n",
    "    model_config.anchor_scale, model_config.image_size\n",
    "    )\n",
    "anchor_labeler = AnchorLabeler(anchors, model_config.num_classes,match_threshold=0.5)\n",
    "loss_func = DetectionLoss(model_config)\n",
    "class_out, box_out = model(b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes, labels = [],[]\n",
    "perm = torch.LongTensor([1,0,3,2])\n",
    "\n",
    "for i in range(b[0].shape[0]):\n",
    "    box, lbl = un_pad(b[1][i],b[2][i])\n",
    "    boxes.append(_unscale_pnts(box[:,perm],256)) # to tf style, yxyx\n",
    "    labels.append(lbl.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_targets, box_targets, num_positivies = anchor_labeler.batch_label_anchors(\n",
    "    b[0].shape[0], boxes, labels\n",
    ")\n",
    "loss, class_loss, box_loss = loss_func(class_out, box_out, cls_targets, box_targets, num_positivies)\n",
    "loss,class_loss,box_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_boxs,t_labels = un_pad(b[1][1],b[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(torch.Size([35, 4]), torch.Size([35]))"
     },
     "metadata": {},
     "execution_count": 125
    }
   ],
   "source": [
    "t_boxs.shape,t_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "TensorPoint([[ 42.0000,  98.7500,  62.5000, 117.5000],\n        [ 40.2500,  33.7500,  59.7500,  57.0000],\n        [ 10.7500,  16.7500,  35.2500,  41.7500],\n        [150.7500,  30.5000, 170.2500,  53.0000],\n        [167.0000,  29.7500, 192.5000,  72.7500],\n        [191.5000,  46.2500, 227.0000,  71.7500],\n        [206.2500,  28.0000, 232.7500,  49.5000],\n        [233.7500, 184.0000, 256.0000, 200.0000],\n        [141.2500,  66.5000, 155.7500,  87.0000],\n        [174.5000,  76.2500, 192.5000,  95.0000],\n        [192.2500,  87.0000, 215.0000, 106.5000],\n        [208.0000, 112.0000, 244.7500, 132.7500],\n        [217.2500, 126.2500, 234.5000, 149.2500],\n        [146.5000, 144.5000, 176.5000, 163.2500],\n        [ 64.2500, 133.5000,  88.0000, 159.0000],\n        [ 88.5000, 145.5000, 107.5000, 167.7500],\n        [ 25.2500, 153.7500,  42.7500, 178.2500],\n        [ 37.0000, 236.5000,  70.0000, 255.2500],\n        [116.0000, 194.2500, 139.0000, 222.5000],\n        [116.7500, 184.0000, 143.7500, 200.5000],\n        [155.2500, 214.5000, 180.5000, 230.7500],\n        [195.2500, 214.7500, 219.7500, 237.0000],\n        [192.0000, 240.2500, 225.7500, 256.0000],\n        [220.2500, 151.7500, 234.2500, 163.0000],\n        [221.7500, 180.0000, 234.2500, 194.0000],\n        [236.7500,  95.5000, 256.0000, 109.5000],\n        [240.0000,  73.7500, 256.0000,  91.0000],\n        [216.2500,  71.0000, 233.7500,  86.7500],\n        [234.7500,  54.7500, 254.0000,  75.2500],\n        [ 65.2500,  81.2500,  78.7500, 100.2500],\n        [ 30.0000, 100.7500,  49.5000, 117.0000],\n        [ 80.2500, 177.2500,  98.0000, 189.5000],\n        [ 28.0000, 225.0000,  45.5000, 242.7500],\n        [149.0000,  60.5000, 166.7500,  76.0000],\n        [219.7500, 240.7500, 245.7500, 256.0000]])"
     },
     "metadata": {},
     "execution_count": 135
    }
   ],
   "source": [
    "_unscale_pnts(t_boxs,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "TensorPoint([[ 98.7500,  42.0000, 117.5000,  62.5000],\n        [ 33.7500,  40.2500,  57.0000,  59.7500],\n        [ 16.7500,  10.7500,  41.7500,  35.2500],\n        [ 30.5000, 150.7500,  53.0000, 170.2500],\n        [ 29.7500, 167.0000,  72.7500, 192.5000],\n        [ 46.2500, 191.5000,  71.7500, 227.0000],\n        [ 28.0000, 206.2500,  49.5000, 232.7500],\n        [184.0000, 233.7500, 200.0000, 256.0000],\n        [ 66.5000, 141.2500,  87.0000, 155.7500],\n        [ 76.2500, 174.5000,  95.0000, 192.5000],\n        [ 87.0000, 192.2500, 106.5000, 215.0000],\n        [112.0000, 208.0000, 132.7500, 244.7500],\n        [126.2500, 217.2500, 149.2500, 234.5000],\n        [144.5000, 146.5000, 163.2500, 176.5000],\n        [133.5000,  64.2500, 159.0000,  88.0000],\n        [145.5000,  88.5000, 167.7500, 107.5000],\n        [153.7500,  25.2500, 178.2500,  42.7500],\n        [236.5000,  37.0000, 255.2500,  70.0000],\n        [194.2500, 116.0000, 222.5000, 139.0000],\n        [184.0000, 116.7500, 200.5000, 143.7500],\n        [214.5000, 155.2500, 230.7500, 180.5000],\n        [214.7500, 195.2500, 237.0000, 219.7500],\n        [240.2500, 192.0000, 256.0000, 225.7500],\n        [151.7500, 220.2500, 163.0000, 234.2500],\n        [180.0000, 221.7500, 194.0000, 234.2500],\n        [ 95.5000, 236.7500, 109.5000, 256.0000],\n        [ 73.7500, 240.0000,  91.0000, 256.0000],\n        [ 71.0000, 216.2500,  86.7500, 233.7500],\n        [ 54.7500, 234.7500,  75.2500, 254.0000],\n        [ 81.2500,  65.2500, 100.2500,  78.7500],\n        [100.7500,  30.0000, 117.0000,  49.5000],\n        [177.2500,  80.2500, 189.5000,  98.0000],\n        [225.0000,  28.0000, 242.7500,  45.5000],\n        [ 60.5000, 149.0000,  76.0000, 166.7500],\n        [240.7500, 219.7500, 256.0000, 245.7500]])"
     },
     "metadata": {},
     "execution_count": 136
    }
   ],
   "source": [
    "_unscale_pnts(t_boxs[:,perm],256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[-0.6719, -0.2285, -0.5117, -0.0820],\n        [-0.6855, -0.7363, -0.5332, -0.5547],\n        [-0.9160, -0.8691, -0.7246, -0.6738],\n        [ 0.1777, -0.7617,  0.3301, -0.5859],\n        [ 0.3047, -0.7676,  0.5039, -0.4316],\n        [ 0.4961, -0.6387,  0.7734, -0.4395],\n        [ 0.6113, -0.7812,  0.8184, -0.6133],\n        [ 0.8262,  0.4375,  1.0000,  0.5625],\n        [ 0.1035, -0.4805,  0.2168, -0.3203],\n        [ 0.3633, -0.4043,  0.5039, -0.2578],\n        [ 0.5020, -0.3203,  0.6797, -0.1680],\n        [ 0.6250, -0.1250,  0.9121,  0.0371],\n        [ 0.6973, -0.0137,  0.8320,  0.1660],\n        [ 0.1445,  0.1289,  0.3789,  0.2754],\n        [-0.4980,  0.0430, -0.3125,  0.2422],\n        [-0.3086,  0.1367, -0.1602,  0.3105],\n        [-0.8027,  0.2012, -0.6660,  0.3926],\n        [-0.7109,  0.8477, -0.4531,  0.9941],\n        [-0.0938,  0.5176,  0.0859,  0.7383],\n        [-0.0879,  0.4375,  0.1230,  0.5664],\n        [ 0.2129,  0.6758,  0.4102,  0.8027],\n        [ 0.5254,  0.6777,  0.7168,  0.8516],\n        [ 0.5000,  0.8770,  0.7637,  1.0000],\n        [ 0.7207,  0.1855,  0.8301,  0.2734],\n        [ 0.7324,  0.4062,  0.8301,  0.5156],\n        [ 0.8496, -0.2539,  1.0000, -0.1445],\n        [ 0.8750, -0.4238,  1.0000, -0.2891],\n        [ 0.6895, -0.4453,  0.8262, -0.3223],\n        [ 0.8340, -0.5723,  0.9844, -0.4121],\n        [-0.4902, -0.3652, -0.3848, -0.2168],\n        [-0.7656, -0.2129, -0.6133, -0.0859],\n        [-0.3730,  0.3848, -0.2344,  0.4805],\n        [-0.7812,  0.7578, -0.6445,  0.8965],\n        [ 0.1641, -0.5273,  0.3027, -0.4062],\n        [ 0.7168,  0.8809,  0.9199,  1.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000]])"
     },
     "metadata": {},
     "execution_count": 127
    }
   ],
   "source": [
    "b[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([4, 61, 4])"
     },
     "metadata": {},
     "execution_count": 110
    }
   ],
   "source": [
    "b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([61, 4])"
     },
     "metadata": {},
     "execution_count": 104
    }
   ],
   "source": [
    "t_boxs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[-0.6719, -0.2285, -0.5117, -0.0820],\n        [-0.6855, -0.7363, -0.5332, -0.5547],\n        [-0.9160, -0.8691, -0.7246, -0.6738],\n        [ 0.1777, -0.7617,  0.3301, -0.5859],\n        [ 0.3047, -0.7676,  0.5039, -0.4316],\n        [ 0.4961, -0.6387,  0.7734, -0.4395],\n        [ 0.6113, -0.7812,  0.8184, -0.6133],\n        [ 0.8262,  0.4375,  1.0000,  0.5625],\n        [ 0.1035, -0.4805,  0.2168, -0.3203],\n        [ 0.3633, -0.4043,  0.5039, -0.2578],\n        [ 0.5020, -0.3203,  0.6797, -0.1680],\n        [ 0.6250, -0.1250,  0.9121,  0.0371],\n        [ 0.6973, -0.0137,  0.8320,  0.1660],\n        [ 0.1445,  0.1289,  0.3789,  0.2754],\n        [-0.4980,  0.0430, -0.3125,  0.2422],\n        [-0.3086,  0.1367, -0.1602,  0.3105],\n        [-0.8027,  0.2012, -0.6660,  0.3926],\n        [-0.7109,  0.8477, -0.4531,  0.9941],\n        [-0.0938,  0.5176,  0.0859,  0.7383],\n        [-0.0879,  0.4375,  0.1230,  0.5664],\n        [ 0.2129,  0.6758,  0.4102,  0.8027],\n        [ 0.5254,  0.6777,  0.7168,  0.8516],\n        [ 0.5000,  0.8770,  0.7637,  1.0000],\n        [ 0.7207,  0.1855,  0.8301,  0.2734],\n        [ 0.7324,  0.4062,  0.8301,  0.5156],\n        [ 0.8496, -0.2539,  1.0000, -0.1445],\n        [ 0.8750, -0.4238,  1.0000, -0.2891],\n        [ 0.6895, -0.4453,  0.8262, -0.3223],\n        [ 0.8340, -0.5723,  0.9844, -0.4121],\n        [-0.4902, -0.3652, -0.3848, -0.2168],\n        [-0.7656, -0.2129, -0.6133, -0.0859],\n        [-0.3730,  0.3848, -0.2344,  0.4805],\n        [-0.7812,  0.7578, -0.6445,  0.8965],\n        [ 0.1641, -0.5273,  0.3027, -0.4062],\n        [ 0.7168,  0.8809,  0.9199,  1.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000]])"
     },
     "metadata": {},
     "execution_count": 107
    }
   ],
   "source": [
    "b[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dls.decode_batch(b)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.vision.core import _unscale_pnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "TensorPoint([[ 42.0000,  98.7500,  62.5000, 117.5000],\n        [ 40.2500,  33.7500,  59.7500,  57.0000],\n        [ 10.7500,  16.7500,  35.2500,  41.7500],\n        [150.7500,  30.5000, 170.2500,  53.0000],\n        [167.0000,  29.7500, 192.5000,  72.7500],\n        [191.5000,  46.2500, 227.0000,  71.7500],\n        [206.2500,  28.0000, 232.7500,  49.5000],\n        [233.7500, 184.0000, 256.0000, 200.0000],\n        [141.2500,  66.5000, 155.7500,  87.0000],\n        [174.5000,  76.2500, 192.5000,  95.0000],\n        [192.2500,  87.0000, 215.0000, 106.5000],\n        [208.0000, 112.0000, 244.7500, 132.7500],\n        [217.2500, 126.2500, 234.5000, 149.2500],\n        [146.5000, 144.5000, 176.5000, 163.2500],\n        [ 64.2500, 133.5000,  88.0000, 159.0000],\n        [ 88.5000, 145.5000, 107.5000, 167.7500],\n        [ 25.2500, 153.7500,  42.7500, 178.2500],\n        [ 37.0000, 236.5000,  70.0000, 255.2500],\n        [116.0000, 194.2500, 139.0000, 222.5000],\n        [116.7500, 184.0000, 143.7500, 200.5000],\n        [155.2500, 214.5000, 180.5000, 230.7500],\n        [195.2500, 214.7500, 219.7500, 237.0000],\n        [192.0000, 240.2500, 225.7500, 256.0000],\n        [220.2500, 151.7500, 234.2500, 163.0000],\n        [221.7500, 180.0000, 234.2500, 194.0000],\n        [236.7500,  95.5000, 256.0000, 109.5000],\n        [240.0000,  73.7500, 256.0000,  91.0000],\n        [216.2500,  71.0000, 233.7500,  86.7500],\n        [234.7500,  54.7500, 254.0000,  75.2500],\n        [ 65.2500,  81.2500,  78.7500, 100.2500],\n        [ 30.0000, 100.7500,  49.5000, 117.0000],\n        [ 80.2500, 177.2500,  98.0000, 189.5000],\n        [ 28.0000, 225.0000,  45.5000, 242.7500],\n        [149.0000,  60.5000, 166.7500,  76.0000],\n        [219.7500, 240.7500, 245.7500, 256.0000],\n        [128.0000, 128.0000, 128.0000, 128.0000],\n        [128.0000, 128.0000, 128.0000, 128.0000],\n        [128.0000, 128.0000, 128.0000, 128.0000],\n        [128.0000, 128.0000, 128.0000, 128.0000],\n        [128.0000, 128.0000, 128.0000, 128.0000],\n        [128.0000, 128.0000, 128.0000, 128.0000],\n        [128.0000, 128.0000, 128.0000, 128.0000],\n        [128.0000, 128.0000, 128.0000, 128.0000],\n        [128.0000, 128.0000, 128.0000, 128.0000],\n        [128.0000, 128.0000, 128.0000, 128.0000],\n        [128.0000, 128.0000, 128.0000, 128.0000],\n        [128.0000, 128.0000, 128.0000, 128.0000],\n        [128.0000, 128.0000, 128.0000, 128.0000],\n        [128.0000, 128.0000, 128.0000, 128.0000],\n        [128.0000, 128.0000, 128.0000, 128.0000],\n        [128.0000, 128.0000, 128.0000, 128.0000],\n        [128.0000, 128.0000, 128.0000, 128.0000],\n        [128.0000, 128.0000, 128.0000, 128.0000],\n        [128.0000, 128.0000, 128.0000, 128.0000],\n        [128.0000, 128.0000, 128.0000, 128.0000],\n        [128.0000, 128.0000, 128.0000, 128.0000],\n        [128.0000, 128.0000, 128.0000, 128.0000],\n        [128.0000, 128.0000, 128.0000, 128.0000],\n        [128.0000, 128.0000, 128.0000, 128.0000],\n        [128.0000, 128.0000, 128.0000, 128.0000],\n        [128.0000, 128.0000, 128.0000, 128.0000]])"
     },
     "metadata": {},
     "execution_count": 96
    }
   ],
   "source": [
    "_unscale_pnts(b[1][1],256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = torch.LongTensor([1,0,3,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([ 95.0000,   0.0000, 108.7500,   3.2500])"
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "source": [
    "_unscale_pnts(b[1],256)[:,:,perm][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from effdet.anchors import Anchors,AnchorLabeler,generate_detections,MAX_DETECTION_POINTS\n",
    "from effdet.loss import DetectionLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = Anchors(\n",
    "    model_config.min_level,model_config.max_level,\n",
    "    model_config.num_scales, model_config.aspect_ratios,\n",
    "    model_config.anchor_scale, model_config.image_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_labeler = AnchorLabeler(anchors, model_config.num_classes,match_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = DetectionLoss(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(b[0])\n",
    "class_out, box_out = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4"
     },
     "metadata": {},
     "execution_count": 129
    }
   ],
   "source": [
    "b[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes, labels = [],[]\n",
    "perm = torch.LongTensor([1,0,3,2])\n",
    "\n",
    "for i in range(b[0].shape[0]):\n",
    "    box, lbl = un_pad(b[1][i],b[2][i])\n",
    "    boxes.append(_unscale_pnts(box[:,perm],256)) # to tf style, yxyx\n",
    "    labels.append(lbl.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_targets, box_targets, num_positivies = anchor_labeler.batch_label_anchors(\n",
    "    b[0].shape[0], boxes, labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, class_loss, box_loss = loss_func(class_out, box_out, cls_targets, box_targets, num_positivies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(tensor(19.9048, grad_fn=<AddBackward0>),\n tensor(17.6527, grad_fn=<SumBackward1>),\n tensor(0.0450, grad_fn=<SumBackward1>))"
     },
     "metadata": {},
     "execution_count": 145
    }
   ],
   "source": [
    "loss,class_loss,box_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(20.1627, grad_fn=<AddBackward0>)"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(17.5499, grad_fn=<SumBackward1>)"
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "class_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(0.0523, grad_fn=<SumBackward1>)"
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "box_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([4, 2, 2, 36])"
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "box_targets[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(torch.Size([4, 3, 256, 256]), torch.Size([4, 66, 4]), torch.Size([4, 66]))"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "b1 = dls.one_batch()\n",
    "b1[0].shape,b1[1].shape,b1[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([4, 9, 32, 32])\ntorch.Size([4, 9, 16, 16])\ntorch.Size([4, 9, 8, 8])\ntorch.Size([4, 9, 4, 4])\ntorch.Size([4, 9, 2, 2])\n"
    }
   ],
   "source": [
    "for p in preds[0]:\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([4, 36, 32, 32])\ntorch.Size([4, 36, 16, 16])\ntorch.Size([4, 36, 8, 8])\ntorch.Size([4, 36, 4, 4])\ntorch.Size([4, 36, 2, 2])\n"
    }
   ],
   "source": [
    "for p in preds[1]:\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitfastai2conda083c600ae6d54f50a73a18c04ce04a99",
   "display_name": "Python 3.7.7 64-bit ('fastai2': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}